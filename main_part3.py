# -*- coding: utf-8 -*-
"""main_part3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aS-VBWwbuu6pOjW37sJfhU1-2qNKZ3H
"""

import numpy as np
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
california_housing = fetch_california_housing()
X, y = california_housing.data, california_housing.target



california_housing.target

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# Split the data into two halves for training and validation purposes
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Split the remaining training data into a subset for validation and the rest for training
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

# Print the shapes of the resulting subsets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# i)number of hidden layers and number of neural units for each layer;
# The number of hidden layers and the number of neural units in each layer determine the complexity and 
# capacity of the model. More layers and units can help the model learn more complex patterns and relationships in the data, 
# but too many can lead to overfitting. 

# ii)optimizer, 
# Here are some of the strengths and weaknesses of the three popular optimizers: Stochastic Gradient Descent (SGD), Adam, and RMSprop.

# Stochastic Gradient Descent (SGD):
# Strengths:

# Computationally efficient and easy to implement.
# Can converge to a good solution when the learning rate is set correctly.
# Performs well on small and noisy datasets.
# Weaknesses:

# Can get stuck in local minima when the learning rate is too high or too low.
# Learning rate scheduling can be tricky to set up.
# Can have high variance in the gradient estimates due to the use of mini-batches.

# Adam:
# Strengths:

# Combines the benefits of AdaGrad (adaptive learning rates) and RMSprop (moving average of gradients).
# Efficient and easy to implement.
# Performs well on large and noisy datasets.
# Converges faster than SGD in many cases.
# Weaknesses:

# Can overfit the data if the learning rate is not tuned correctly.
# Can get stuck in saddle points or plateaus.

# RMSprop:
# Strengths:

# Adapts the learning rate for each parameter based on the history of its gradients.
# Performs well on non-stationary or sparse problems.
# Can converge faster than SGD in some cases.
# Weaknesses:

# Can have high variance in the gradient estimates when the learning rate is too high or too low.
# Can get stuck in local minima when the learning rate is not tuned correctly.
# Does not perform as well on problems with a large number of parameters or a high level of noise.

# iii)activation function (sigm, tanh_opt) and 
# Sigmoid squashes the output to a range between 0 and 1, while tanh squashes the output to a range between -1 and 1. 
# Sigmoid (sigm):
# Strengths:

# Squashes the output to a range between 0 and 1, which can be useful for binary classification problems.
# Can be interpreted as a probability or confidence measure for the output.
# Easy to understand and implement.
# Weaknesses:

# Saturates at large or small inputs, which can lead to vanishing gradients and slow learning.
# Not zero-centered, which can cause issues in backpropagation.
# Hyperbolic tangent (tanh_opt):
# Strengths:

# Squashes the output to a range between -1 and 1, which can be useful for symmetric data.
# Zero-centered, which can help with backpropagation.
# More expressive than sigmoid due to its steeper slope and wider range.
# Weaknesses:

# Saturates at large or small inputs, which can lead to vanishing gradients and slow learning.
# Not always suitable for all types of data, particularly data that is already normalized.
# Both sigmoid and tanh activation functions are prone to the vanishing gradient problem, 
# which can slow down learning or even prevent the network from learning entirely. 
# This issue can be mitigated by using other activation functions like ReLU, Leaky ReLU, or ELU. 
# The choice of activation function depends on the type of problem and the characteristics of the data.

# iv)output function (‘sigm’,’linear’, ‘softmax’); 
# sigmoid (sigm) is commonly used for binary classification problems, 
# linear is used for regression problems, 
# and softmax is used for multiclass classification problems.

# Set 1
# Set hyperparameters
import tensorflow as tf
hidden_layer_sizes = [64, 32]
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
activation = 'sigmoid'
output = 'linear'

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score


# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build FNN model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_sizes[0], activation=activation, input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dense(hidden_layer_sizes[1], activation=activation),
    #tf.keras.layers.Dense(hidden_layer_sizes[2], activation=activation),
    tf.keras.layers.Dense(1, activation=output)
])

# Compile model
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train model
model.fit(X_train, y_train,epochs=50, batch_size=32)

# Evaluate the model on the validation set
y_pred_set1 = model.predict(X_val)
r2_set1 = r2_score(y_val, y_pred_set1)
print("R2 Score: ", r2_set1)

# Set 2
# Set hyperparameters
hidden_layer_sizes = [100,50]
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
activation = 'sigmoid'
output = 'linear'

# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build FNN model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_sizes[0], activation=activation, input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dense(hidden_layer_sizes[1], activation=activation),
    #tf.keras.layers.Dense(hidden_layer_sizes[2], activation=activation),
    tf.keras.layers.Dense(1, activation=output)
])

# Compile model
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train model
model.fit(X_train, y_train,epochs=50, batch_size=32)

# Evaluate the model on the validation set
y_pred_set2 = model.predict(X_val)
r2_set2 = r2_score(y_val, y_pred_set2)
print("R2 Score: ", r2_set2)

y_pred_set2.shape

# Set 3
# Define the hyperparameters
hidden_layer_sizes = [100,50]
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
activation = 'sigmoid'
output = 'linear'

# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build FNN model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_sizes[0], activation=activation, input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dense(hidden_layer_sizes[1], activation=activation),
    #tf.keras.layers.Dense(hidden_layer_sizes[2], activation=activation),
    tf.keras.layers.Dense(1, activation=output)
])

# Compile model
from tensorflow.keras import metrics
model.compile(optimizer=optimizer, loss='mse', metrics=[metrics.MeanAbsoluteError(name='mae'), metrics.CategoricalAccuracy(name='accuracy')])

# Train model
model.fit(X_train, y_train,epochs=50, batch_size=32)

# Evaluate the model on the validation set
y_pred_set3 = model.predict(X_val)
r2_set3 = r2_score(y_val, y_pred_set3)
print("R2 Score: ", r2_set3)

# Test
# Define the hyperparameters
hidden_layer_sizes = [100,50]
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
activation = 'sigmoid'
output = 'linear'

# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build FNN model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_sizes[0], activation=activation, input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dense(hidden_layer_sizes[1], activation=activation),
    #tf.keras.layers.Dense(hidden_layer_sizes[2], activation=activation),
    tf.keras.layers.Dense(1, activation=output)
])

# Compile model
from tensorflow.keras import metrics
model.compile(optimizer=optimizer, loss='mse', metrics=[metrics.MeanAbsoluteError(name='mae'), metrics.CategoricalAccuracy(name='accuracy')])

# Train model
model.fit(X_train, y_train,epochs=50, batch_size=32)

# Evaluate the model on the validation set
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print("R2 Score: ", r2)

# Use cross validation to dealing with negative R^2 problem
from sklearn.model_selection import KFold
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras import metrics

# Define FNN model
def create_model():
    model = models.Sequential([
        layers.Dense(hidden_layer_sizes[0], activation=activation, input_shape=[X_train.shape[1]]),
        layers.Dense(hidden_layer_sizes[1], activation=activation),
        layers.Dense(1, activation=output)
    ])
    model.compile(optimizer=optimizers.Adam(learning_rate=0.01), loss='mse', metrics=[metrics.MeanAbsoluteError(name='mae')])
    return model

# Define cross-validation
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Train and evaluate the model using cross-validation
r2_scores = []
for i, (train_index, val_index) in enumerate(kf.split(X_train)):
    print(f"Fold {i+1}/{k}")
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]
    
    # Create a new model for each fold
    model = create_model()
    
    # Train the model on the training set for this fold
    model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, verbose=0)
    
    # Evaluate the model on the validation set for this fold
    y_val_pred = model.predict(X_val_fold)
    r2 = r2_score(y_val_fold, y_val_pred)
    print(f"R2 Score: {r2}")
    r2_scores.append(r2)
    
# Evaluate the model on the test set
y_test_pred = model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"R2 Score on test set: {r2_test}")

# Compute the average R2 score over all folds
mean_r2 = np.mean(r2_scores)
print(f"Mean R2 Score: {mean_r2}")

y_pred.shape

